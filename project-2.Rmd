---
title: "STA 521 Project 2"
author: "Eli Gnesin (ejg45) & Caleb Woo"
date: "2022-12-06"
number-sections: true
bibliography: references.bib
output: pdf_document
---

# Data Collection and Exploration

```{r setup-packages, echo=FALSE, message=FALSE}
library(tidyverse)
library(GGally)
library(ggplot2)
library(lubridate)
library(magrittr)
library(gridExtra)
library(latex2exp)
library(stats)
library(plotly)
library(rjson)
library(kableExtra)
library(knitr)
library(kableExtra)
library(caret)
library(tree)
library(rpart)

source("CVmaster.R")

knitr::opts_chunk$set(fig.align="center", echo=FALSE, eval=TRUE,
                      message=FALSE, warning=FALSE, error=FALSE)
```

```{r read-data}
columns = c("image", "Y", "X", "Label", "NDAI", "SD", "CORR", 
            "DF", "CF", "BF", "AF", "AN")

image1 = read.table("image_data/imagem1.txt", header=FALSE)
image1 = cbind(rep(1, nrow(image1)), image1)
colnames(image1) = columns

image2 = read.table("image_data/imagem2.txt", header=FALSE)
image2 = cbind(rep(2, nrow(image2)), image2)
colnames(image2) = columns

image3 = read.table("image_data/imagem3.txt", header=FALSE)
image3 = cbind(rep(3, nrow(image3)), image3)
colnames(image3) = columns

data = rbind(image1, image2, image3)
```

## Summary of the Paper

In 2008, Yu et al. published "Daytime Arctic Cloud Detection Based on Multi-Angle Satellite Data With Case Studies" [@shi_yu_clothiaux_braverman_2008]. The purpose of the study was to explore the state of daytime Arctic cloud classification, and propose a new "Enhanced Linear Correlation Matching Algorithm" and classification system that is less reliant on manual human classification.

The data used in the study was collected by the Multiangle Imaging SpectroRadiometer (MISR) on NASA's Terra satellite, and for this study, 10 orbits of the satellite path over the Arctic, northern Greenland, and Baffin Bay, collected between April and September 2002, were used. Together, this dataset comprises 57 "data units" of three MISR blocks each (with a block being $\frac{1}{180}$ of a path), totaling 7.11 million pixels of 1.1 kilometer resolution (as well as 275 meter resolution for red radiation measurements). The labels of cloud cover and no-cloud cover (as well as an "ambiguous" label), were provided by industry experts.

The research team then used the data to construct three "physical features," CORR, SD, NDAI. CORR is the average of the linear correlations of two pairs of radiation measurements (Af/An and Bf/An), where higher values suggest a cloud-free image. The second, SD, is the standard deviation of the An radiation measurements, used to help detect "smooth cloud-free surfaces", and NDAI is the average of two radiation measurements over a 1.1 kilometer spacial resolution. Using these features, the research team created two decision rules to label a 1.1 km square pixel as clear, and with setting appropriate thresholds, reached over 91% agreement with the expert labels and 100% coverage in labeling. From this, the team demonstrated that the three physical features were sufficient to accurately classify cloud cover in arctic environments with better spatial coverage and real-time adaptive thresholding to improve the robustness of the model. Furthermore, the ELCM algorithm was used to train QDA to provide probability labels for partly cloudy scenes and was found to be effective in identifying cloud boundaries. By improving understanding of the flow of radiation through the atmosphere and how clouds respond to changes in arctic climate, this study is the first step towards analyzing how changing cloud properties may improve or destroy the changes in the Arctic brought about by climate change.

## Summary of the Data

```{r summary-pixels}
class_count <- data %>%
  count(Label)
class_count$Label <- c("Not Cloud", "Unlabeled", "Cloud")
class_count$n <- 100*(class_count$n/nrow(data))
names(class_count)[2] <- "% of pixels"
class_count %>%
  kbl(digits = 3) %>%
  kable_styling(latex_options = "HOLD_position", full_width = F)
```

```{r maps}
map1 <- ggplot(data=image1, aes(x=X, y=Y, color=factor(Label))) +
  geom_point() +
  coord_quickmap(expand = F, clip = "on") +
  labs(title = "Image 1") +
  scale_color_manual(labels = c("Not Cloud", "Unlabeled", "Cloud"),
                     values = c("chocolate3", "darkseagreen", "deepskyblue"),
                     name = "Label") +
  theme(legend.position = "none")
map2 <- ggplot(data=image2, aes(x=X, y=Y, color=factor(Label))) +
  geom_point() +
  coord_quickmap(expand = F, clip = "on") +
  labs(title = "Image 2") +
  scale_color_manual(labels = c("Not Cloud", "Unlabeled", "Cloud"),
                     values = c("chocolate3", "darkseagreen", "deepskyblue"),
                     name = "Label") +
  theme(legend.position = "none")
map3 <- ggplot(data=image3, aes(x=X, y=Y, color=factor(Label))) +
  geom_point() +
  coord_quickmap(expand = F, clip = "on") +
  labs(title = "Image 3") +
  scale_color_manual(labels = c("Not Cloud", "Unlabeled", "Cloud"),
                     values = c("chocolate3", "darkseagreen", "deepskyblue"),
                     name = "Label")
grid.arrange(map1, map2, map3, nrow=1, widths=c(1, 1, 1.65))
```

## Exploratory Data Analysis

We now continue with a short exploration of the data itself. First, we look at the pairwise correlations between the features, excluding the coordinates:

```{r correlations}

cor(data[,4:12], data[,4:12]) %>%
  kable(format = "simple", digits =3)
```

From these correlations, some trends stand out. All three of the "physical features" in the paper (NDAI, SD, CORR), are positively correlated with label, which indicates that *higher* values of all three of these features are associated with a label of +1, meaning cloud cover. In contrast, however, four of the five given radiance angles show negative correlation with the label variable, indicating that *lower* values of radiance are generally associated with cloud cover. We can also compare differences between the features in the two classes by considering the densities of the features separated by label:

```{r density-plots, fig.height=3, fig.width=8}
colors = c("clouds" = "deepskyblue", 
           "noclouds" = "chocolate3",
           "other" = "darkseagreen")

g1 = ggplot() +
  geom_density(aes(x = NDAI, color = "clouds", fill = "clouds"), 
               data = data %>% filter(Label == 1), 
               alpha = 0.6) +
    geom_density(aes(x = NDAI, color = "noclouds", fill = "noclouds"), 
               data = data %>% filter(Label == -1), 
               alpha = 0.6) +
    geom_density(aes(x = NDAI, color = "other", fill = "other"), 
               data = data %>% filter(Label == 0), 
               alpha = 0.6) +
  labs(title = "NDAI",
       x = element_blank(),
       y = element_blank()) +
  scale_fill_manual(values = colors) +
  scale_color_manual(values = colors) +
  theme_minimal() + 
  theme(legend.position = "none")

g2 = ggplot() +
  geom_density(aes(x = log(SD), color = "clouds", fill = "clouds"), 
               data = data %>% filter(Label == 1), 
               alpha = 0.6) +
    geom_density(aes(x = log(SD), color = "noclouds", fill = "noclouds"), 
               data = data %>% filter(Label == -1), 
               alpha = 0.6) +
    geom_density(aes(x = log(SD), color = "other", fill = "other"), 
               data = data %>% filter(Label == 0), 
               alpha = 0.6) +
  labs(title = "log(SD)",
       x = element_blank(),
       y = element_blank()) +
  scale_fill_manual(values = colors) +
  scale_color_manual(values = colors) +
  theme_minimal() + 
  theme(legend.position = "none")

g3 = ggplot() +
  geom_density(aes(x = CORR, color = "clouds", fill = "clouds"), 
               data = data %>% filter(Label == 1), 
               alpha = 0.6) +
    geom_density(aes(x = CORR, color = "noclouds", fill = "noclouds"), 
               data = data %>% filter(Label == -1), 
               alpha = 0.6) +
    geom_density(aes(x = CORR, color = "other", fill = "other"), 
               data = data %>% filter(Label == 0), 
               alpha = 0.6) +
  labs(title = "CORR",
       x = element_blank(),
       y = element_blank()) +
  scale_fill_manual(values = colors) +
  scale_color_manual(values = colors) +
  theme_minimal() + 
  theme(legend.position = "none")

g4 = ggplot() +
  geom_density(aes(x = DF, color = "clouds", fill = "clouds"), 
               data = data %>% filter(Label == 1), 
               alpha = 0.6) +
    geom_density(aes(x = DF, color = "noclouds", fill = "noclouds"), 
               data = data %>% filter(Label == -1), 
               alpha = 0.6) +
    geom_density(aes(x = DF, color = "other", fill = "other"), 
               data = data %>% filter(Label == 0), 
               alpha = 0.6) +
  labs(title = "DF",
       x = element_blank(),
       y = element_blank()) +
  scale_fill_manual(values = colors) +
  scale_color_manual(values = colors) +
  theme_minimal() + 
  theme(legend.position = "none")

g5 = ggplot() +
  geom_density(aes(x = CF, color = "clouds", fill = "clouds"), 
               data = data %>% filter(Label == 1), 
               alpha = 0.6) +
    geom_density(aes(x = CF, color = "noclouds", fill = "noclouds"), 
               data = data %>% filter(Label == -1), 
               alpha = 0.6) +
    geom_density(aes(x = CF, color = "other", fill = "other"), 
               data = data %>% filter(Label == 0), 
               alpha = 0.6) +
  labs(title = "CF",
       x = element_blank(),
       y = element_blank()) +
  scale_fill_manual(values = colors) +
  scale_color_manual(values = colors) +
  theme_minimal() + 
  theme(legend.position = "none")

g6 = ggplot() +
  geom_density(aes(x = BF, color = "clouds", fill = "clouds"), 
               data = data %>% filter(Label == 1), 
               alpha = 0.6) +
    geom_density(aes(x = BF, color = "noclouds", fill = "noclouds"), 
               data = data %>% filter(Label == -1), 
               alpha = 0.6) +
    geom_density(aes(x = BF, color = "other", fill = "other"), 
               data = data %>% filter(Label == 0), 
               alpha = 0.6) +
  labs(title = "BF",
       x = element_blank(),
       y = element_blank()) +
  scale_fill_manual(values = colors) +
  scale_color_manual(values = colors) +
  theme_minimal() + 
  theme(legend.position = "none")

g7 = ggplot() +
  geom_density(aes(x = AF, color = "clouds", fill = "clouds"), 
               data = data %>% filter(Label == 1), 
               alpha = 0.6) +
    geom_density(aes(x = AF, color = "noclouds", fill = "noclouds"), 
               data = data %>% filter(Label == -1), 
               alpha = 0.6) +
    geom_density(aes(x = AF, color = "other", fill = "other"), 
               data = data %>% filter(Label == 0), 
               alpha = 0.6) +
  labs(title = "AF",
       x = element_blank(),
       y = element_blank()) +
  scale_fill_manual(values = colors) +
  scale_color_manual(values = colors) +
  theme_minimal() + 
  theme(legend.position = "none")

g8 = ggplot() +
  geom_density(aes(x = AN, color = "clouds", fill = "clouds"), 
               data = data %>% filter(Label == 1), 
               alpha = 0.6) +
    geom_density(aes(x = AN, color = "noclouds", fill = "noclouds"), 
               data = data %>% filter(Label == -1), 
               alpha = 0.6) +
    geom_density(aes(x = AN, color = "other", fill = "other"), 
               data = data %>% filter(Label == 0), 
               alpha = 0.6) +
  labs(title = "AN",
       x = element_blank(),
       y = element_blank()) +
  scale_fill_manual(values = colors) +
  scale_color_manual(values = colors) +
  theme_minimal() + 
  theme(legend.position = "none")

grid.arrange(g1,g2,g3,g4,g5,g6,g7,g8,nrow=2)
```
These density plots reinforce the same ideas seen in the correlations above. In the three "physical features", the "cloud" labeled points have densities higher than the "non-cloud" labeled points, and conversely, for four of the five radiances (all except for DF), the bulk of the density for the "non-cloud" labeled points is above that of the cloud labeled points. Interestingly, the radiance densities for the "cloud" labeled points are generally unimodal, whereas the radiances for the "non-cloud" labeled points are generally bimodal, with the larger mode above the cloud density mode and the smaller mode below. This helps explain the need for the physical features; the radiances are less uniformly separable, and thus have less predictive power than the physical features. 

# Preparation

## Data Split

In splitting the dataset into training, validation, and test subsets, it is important to preserve the structure of the data, that is, that the observations are denoted by their X and Y spatial coordinates, and any split of the data would have to preserve this spatial structure. One method for doing so is to carve each image up into some $k$ blocks, and then to randomly permute the block identifiers, the range of numbers from 1 to $3k$, and then take a split from the random block permutations. Such a method does preserve the spatial structure of the data, as each point is in a set with some (or all if not an edge) of the points around it. The random permutation of the blocks also ensures that, while the spatial structure of the points relative to each other is preserved, the structure of the image is not considered in the model, such that the orientation and structure of a individual image is a considered feature of the model.

## Baseline

We can now split the data using the methods outlined above and run a "classifier" which just sets all points to label $-1$, indicating cloudlessness. For the block splitting method, we split each image into 64 blocks, for a total of 192 blocks, of which 30 are for testing, 30 are for validation, and 132 are for training. This method gives a validation set misclassification rate of $63.0\%$ and a test set misclassification rate of $65.3\%$, which is about in line with the overall rate of $-1$ labels being $35-36\%$. This classifier would yield high accuracy if the image data was unbalanced to be mostly cloudless images, but in a scenario where the majority of points are not labeled $-1$, it is not a particularly accurate classifier.

```{r}
set.seed(521)

tl = data %>%
  pull(Label)
td = data %>%
  select(-Label)


td$block = rep(0, nrow(td))
labels_block = numeric()
ims = distinct(td %>% select(image)) %>% pull(image)
snum = 0
  
for (im in ims){
  xvals = td %>%
    filter(image == im) %>%
    pull(X) %>%
    unique()
  yvals = td %>%
    filter(image == im) %>%
    pull(Y) %>%
    unique()
  
  xsplit = split(xvals,
                 cut(seq_along(xvals),
                     8,
                     labels = FALSE))
  
  ysplit = split(yvals,
                 cut(seq_along(yvals),
                     8,
                     labels = FALSE))

  for (xs in xsplit){
    for (ys in ysplit){
      snum = snum + 1
      td = td %>%
        mutate(block = ifelse(
          ((image == im) & (X %in% xs) & (Y %in% ys)), 
          snum, block))
      labels_block = c(labels_block, 
                            rep(snum, 
                                nrow(td %>%
                                       filter((image == im) & 
                                                (X %in% xs) & 
                                                (Y %in% ys)))))
    }
  }
}

#Splitting the blocks
bks = sample(seq.int(from = 1, to = snum, by = 1))

train = tl[labels_block %in% bks[1:132]]
valid = tl[labels_block %in% bks[133:162]]
testy = tl[labels_block %in% bks[163:192]]

invisible(mean(valid != -1))
invisible(mean(testy != -1))

```


## First Order Importance

## Generic Cross-Validation

We then wrote a function that allows us to pass in the name of a trivial classifier (or a function call to such a classifier), and a number of folds, and outputs the $k$-fold Cross-validation misclassification rate on the training data provided using the classifier. The function uses the block splitting method described in Section 2(a), with $k^2$ blocks per image. It then partitions the blocks into $k$ groups, and, for each partitions, treats that partition as a test set while training the classifier on the other $k-1$ partitions. Finally, the function returns the mean of the misclassification rates of the classifiers predicting on each of the $k$ test partitions. The function also allows for optional arguments, which are then passed to the generic classifier, so as to allow for classifiers to be modified from their generic versions.

```{r}
tl = data %>%
  pull(Label)
td = data %>%
  select(-Label)
CVmaster(td, tl, "rpart", method = "class")
```

# Modeling

# Diagnostics

# References
