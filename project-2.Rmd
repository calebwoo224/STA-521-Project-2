---
title: "STA 521 Project 2"
author: "Eli Gnesin (ejg45) & Caleb Woo (csw57)"
date: "2022-12-06"
number-sections: true
bibliography: references.bib
output: pdf_document
---

# Data Collection and Exploration

```{r setup-packages, echo=FALSE, message=FALSE}
library(tidyverse)
library(GGally)
library(ggplot2)
library(lubridate)
library(magrittr)
library(gridExtra)
library(latex2exp)
library(stats)
library(plotly)
library(rjson)
library(kableExtra)
library(knitr)
library(caret)
library(tree)
library(rpart)
library(tidymodels)
library(MASS)
library(tidyr)
library(randomForest)
library(naivebayes)
library(pROC)
library(gbm)
library(rlist)

source("CVmaster.R")
source("image_split.R")
source("block_split.R")

knitr::opts_chunk$set(fig.align="center", echo=FALSE, eval=TRUE,
                      message=FALSE, warning=FALSE, error=FALSE)
```

```{r read-data}
columns = c("image", "Y", "X", "Label", "NDAI", "SD", "CORR", 
            "DF", "CF", "BF", "AF", "AN")

image1 = read.table("image_data/imagem1.txt", header=FALSE)
image1 = cbind(rep(1, nrow(image1)), image1)
colnames(image1) = columns

image2 = read.table("image_data/imagem2.txt", header=FALSE)
image2 = cbind(rep(2, nrow(image2)), image2)
colnames(image2) = columns

image3 = read.table("image_data/imagem3.txt", header=FALSE)
image3 = cbind(rep(3, nrow(image3)), image3)
colnames(image3) = columns

data = rbind(image1, image2, image3)
```

## Summary of the Paper

In 2008, Yu et al. published "Daytime Arctic Cloud Detection Based on Multi-Angle Satellite Data With Case Studies" [@shi_yu_clothiaux_braverman_2008]. The purpose of the study was to explore the state of daytime Arctic cloud classification, and propose a new "Enhanced Linear Correlation Matching Algorithm" and classification system that is less reliant on manual human classification.

The data used in the study was collected by the Multiangle Imaging SpectroRadiometer (MISR) on NASA's Terra satellite, and for this study, 10 orbits of the satellite path over the Arctic, northern Greenland, and Baffin Bay, collected between April and September 2002, were used. Together, this dataset comprises 57 "data units" of three MISR blocks each (with a block being $\frac{1}{180}$ of a path), totaling 7.11 million pixels of 1.1 kilometer resolution (as well as 275 meter resolution for red radiation measurements). The labels of cloud cover and no-cloud cover (as well as an "ambiguous" label), were provided by industry experts.

The research team then used the data to construct three "physical features," CORR, SD, NDAI. CORR is the average of the linear correlations of two pairs of radiation measurements (Af/An and Bf/An), where higher values suggest a cloud-free image. The second, SD, is the standard deviation of the An radiation measurements, used to help detect "smooth cloud-free surfaces", and NDAI is the average of two radiation measurements over a 1.1 kilometer spacial resolution. Using these features, the research team created two decision rules to label a 1.1 km square pixel as clear, and with setting appropriate thresholds, reached over 91% agreement with the expert labels and 100% coverage in labeling. From this, the team demonstrated that the three physical features were sufficient to accurately classify cloud cover in arctic environments with better spatial coverage and real-time adaptive thresholding to improve the robustness of the model. Furthermore, the ELCM algorithm was used to train QDA to provide probability labels for partly cloudy scenes and was found to be effective in identifying cloud boundaries. By improving understanding of the flow of radiation through the atmosphere and how clouds respond to changes in arctic climate, this study is the first step towards analyzing how changing cloud properties may improve or destroy the changes in the Arctic brought about by climate change.

## Summary of the Data

```{r summary-pixels}
class_count <- data %>%
  count(Label)
class_count$Label <- c("Not Cloud", "Unlabeled", "Cloud")
class_count$n <- 100*(class_count$n/nrow(data))
names(class_count)[2] <- "% of pixels"
class_count %>%
  kbl(digits = 3) %>%
  kable_styling(latex_options = "HOLD_position", full_width = F)
```

```{r maps}
map1 <- ggplot(data=image1, aes(x=X, y=Y, color=factor(Label))) +
  geom_point() +
  coord_quickmap(expand = F, clip = "on") +
  labs(title = "Image 1") +
  scale_color_manual(labels = c("Not Cloud", "Unlabeled", "Cloud"),
                     values = c("chocolate3", "darkseagreen", "deepskyblue"),
                     name = "Label") +
  theme(legend.position = "none")
map2 <- ggplot(data=image2, aes(x=X, y=Y, color=factor(Label))) +
  geom_point() +
  coord_quickmap(expand = F, clip = "on") +
  labs(title = "Image 2") +
  scale_color_manual(labels = c("Not Cloud", "Unlabeled", "Cloud"),
                     values = c("chocolate3", "darkseagreen", "deepskyblue"),
                     name = "Label") +
  theme(legend.position = "none")
map3 <- ggplot(data=image3, aes(x=X, y=Y, color=factor(Label))) +
  geom_point() +
  coord_quickmap(expand = F, clip = "on") +
  labs(title = "Image 3") +
  scale_color_manual(labels = c("Not Cloud", "Unlabeled", "Cloud"),
                     values = c("chocolate3", "darkseagreen", "deepskyblue"),
                     name = "Label")

leg = ggpubr::get_legend(map3)

map3 = map3 +
  theme(legend.position = "none")

grid.arrange(map1, map2, map3, leg, nrow=1)
```

Similar to the ELCM-QDA results from the paper, we observe many ambiguous and unlabeled points around the cloud boundaries which separate the cloud pixels from the non-cloud pixels. An i.i.d. assumption for the samples cannot be justified for this dataset because individual pixels seem to depend on the pixels around it. If the data appeared to be i.i.d. we would expect a random scatter of pixels at any given region of an image. However, we observe clear spatial trends where cloud pixels are near other cloud pixels, unlabeled pixels border the clouds, and non-cloud pixels are near other non-cloud pixels. Therefore, individual pixels are essentially meaningless outside of the context of nearby pixels so we cannot make an i.i.d. assumption on the data.

## Exploratory Data Analysis

We now continue with a short exploration of the data itself. First, we look at the pairwise correlations between the features, excluding the coordinates:

```{r corrplot}
corrplot::corrplot(cor(data[,4:12]), type="upper", tl.col="black")
```

From these correlations, some trends stand out. All three of the "physical features" in the paper (NDAI, SD, CORR), are positively correlated with label, which indicates that *higher* values of all three of these features are associated with a label of +1, meaning cloud cover. In contrast, however, four of the five given radiance angles show negative correlation with the label variable, indicating that *lower* values of radiance are generally associated with cloud cover. We can also compare differences between the features in the two classes by considering the densities of the features separated by label:

```{r density-plots, fig.height=3, fig.width=8}
colors = c("clouds" = "deepskyblue", 
           "noclouds" = "chocolate3",
           "other" = "darkseagreen")

g1 = ggplot() +
  geom_density(aes(x = NDAI, color = "clouds", fill = "clouds"), 
               data = data %>% filter(Label == 1), 
               alpha = 0.6) +
    geom_density(aes(x = NDAI, color = "noclouds", fill = "noclouds"), 
               data = data %>% filter(Label == -1), 
               alpha = 0.6) +
    geom_density(aes(x = NDAI, color = "other", fill = "other"), 
               data = data %>% filter(Label == 0), 
               alpha = 0.6) +
  labs(title = "NDAI",
       x = element_blank(),
       y = element_blank()) +
  scale_fill_manual(values = colors) +
  scale_color_manual(values = colors) +
  theme_minimal() + 
  theme(legend.position = "none")

g2 = ggplot() +
  geom_density(aes(x = log(SD), color = "clouds", fill = "clouds"), 
               data = data %>% filter(Label == 1), 
               alpha = 0.6) +
    geom_density(aes(x = log(SD), color = "noclouds", fill = "noclouds"), 
               data = data %>% filter(Label == -1), 
               alpha = 0.6) +
    geom_density(aes(x = log(SD), color = "other", fill = "other"), 
               data = data %>% filter(Label == 0), 
               alpha = 0.6) +
  labs(title = "log(SD)",
       x = element_blank(),
       y = element_blank()) +
  scale_fill_manual(values = colors) +
  scale_color_manual(values = colors) +
  theme_minimal() + 
  theme(legend.position = "none")

g3 = ggplot() +
  geom_density(aes(x = CORR, color = "clouds", fill = "clouds"), 
               data = data %>% filter(Label == 1), 
               alpha = 0.6) +
    geom_density(aes(x = CORR, color = "noclouds", fill = "noclouds"), 
               data = data %>% filter(Label == -1), 
               alpha = 0.6) +
    geom_density(aes(x = CORR, color = "other", fill = "other"), 
               data = data %>% filter(Label == 0), 
               alpha = 0.6) +
  labs(title = "CORR",
       x = element_blank(),
       y = element_blank()) +
  scale_fill_manual(values = colors) +
  scale_color_manual(values = colors) +
  theme_minimal() + 
  theme(legend.position = "none")

g4 = ggplot() +
  geom_density(aes(x = DF, color = "clouds", fill = "clouds"), 
               data = data %>% filter(Label == 1), 
               alpha = 0.6) +
    geom_density(aes(x = DF, color = "noclouds", fill = "noclouds"), 
               data = data %>% filter(Label == -1), 
               alpha = 0.6) +
    geom_density(aes(x = DF, color = "other", fill = "other"), 
               data = data %>% filter(Label == 0), 
               alpha = 0.6) +
  labs(title = "DF",
       x = element_blank(),
       y = element_blank()) +
  scale_fill_manual(values = colors) +
  scale_color_manual(values = colors) +
  theme_minimal() + 
  theme(legend.position = "none")

g5 = ggplot() +
  geom_density(aes(x = CF, color = "clouds", fill = "clouds"), 
               data = data %>% filter(Label == 1), 
               alpha = 0.6) +
    geom_density(aes(x = CF, color = "noclouds", fill = "noclouds"), 
               data = data %>% filter(Label == -1), 
               alpha = 0.6) +
    geom_density(aes(x = CF, color = "other", fill = "other"), 
               data = data %>% filter(Label == 0), 
               alpha = 0.6) +
  labs(title = "CF",
       x = element_blank(),
       y = element_blank()) +
  scale_fill_manual(values = colors) +
  scale_color_manual(values = colors) +
  theme_minimal() + 
  theme(legend.position = "none")

g6 = ggplot() +
  geom_density(aes(x = BF, color = "clouds", fill = "clouds"), 
               data = data %>% filter(Label == 1), 
               alpha = 0.6) +
    geom_density(aes(x = BF, color = "noclouds", fill = "noclouds"), 
               data = data %>% filter(Label == -1), 
               alpha = 0.6) +
    geom_density(aes(x = BF, color = "other", fill = "other"), 
               data = data %>% filter(Label == 0), 
               alpha = 0.6) +
  labs(title = "BF",
       x = element_blank(),
       y = element_blank()) +
  scale_fill_manual(values = colors) +
  scale_color_manual(values = colors) +
  theme_minimal() + 
  theme(legend.position = "none")

g7 = ggplot() +
  geom_density(aes(x = AF, color = "clouds", fill = "clouds"), 
               data = data %>% filter(Label == 1), 
               alpha = 0.6) +
    geom_density(aes(x = AF, color = "noclouds", fill = "noclouds"), 
               data = data %>% filter(Label == -1), 
               alpha = 0.6) +
    geom_density(aes(x = AF, color = "other", fill = "other"), 
               data = data %>% filter(Label == 0), 
               alpha = 0.6) +
  labs(title = "AF",
       x = element_blank(),
       y = element_blank()) +
  scale_fill_manual(values = colors) +
  scale_color_manual(values = colors) +
  theme_minimal() + 
  theme(legend.position = "none")

g8 = ggplot() +
  geom_density(aes(x = AN, color = "clouds", fill = "clouds"), 
               data = data %>% filter(Label == 1), 
               alpha = 0.6) +
    geom_density(aes(x = AN, color = "noclouds", fill = "noclouds"), 
               data = data %>% filter(Label == -1), 
               alpha = 0.6) +
    geom_density(aes(x = AN, color = "other", fill = "other"), 
               data = data %>% filter(Label == 0), 
               alpha = 0.6) +
  labs(title = "AN",
       x = element_blank(),
       y = element_blank()) +
  scale_fill_manual(values = colors) +
  scale_color_manual(values = colors) +
  theme_minimal() + 
  theme(legend.position = "none")

grid.arrange(g1,g2,g3,g4,g5,g6,g7,g8,nrow=2)
```
These density plots reinforce the same ideas seen in the correlations above. In the three "physical features", the "cloud" labeled points have densities higher than the "non-cloud" labeled points, and conversely, for four of the five radiances (all except for DF), the bulk of the density for the "non-cloud" labeled points is above that of the cloud labeled points. Interestingly, the radiance densities for the "cloud" labeled points are generally unimodal, whereas the radiances for the "non-cloud" labeled points are generally bimodal, with the larger mode above the cloud density mode and the smaller mode below. This helps explain the need for the physical features; the radiances are less uniformly separable, and thus have less predictive power than the physical features. 

# Preparation

## Data Split

In splitting the dataset into training, validation, and test subsets, it is important to preserve the structure of the data, that is, that the observations are denoted by their X and Y spatial coordinates, and any split of the data would have to preserve this spatial structure. One method for doing so is to carve each image up into some $k$ blocks, and then to randomly permute the block identifiers, the range of numbers from 1 to $3k$, and then take a split from the random block permutations. Such a method does preserve the spatial structure of the data, as each point is in a set with some (or all if not an edge) of the points around it. The random permutation of the blocks also ensures that, while the spatial structure of the points relative to each other is preserved, the structure of the image is not considered in the model, such that the orientation and structure of a individual image is a considered feature of the model.

A second method for splitting the data into three sets is to assign all data from images 1 and 3 to the training data. Then we can split the image 2 data in half to assign each half to the validation and test sets respectively. By doing so, we retain the majority of the data for training to better fit our classifiers. In addition, by splitting image 2 in half, we still retain the spatial structure of the data for the validation and test sets. We believe that image 2 is the best choice to split in half for our validation and test sets because each half contains numerous samples from each class. Looking at the maps of the validation and test sets below, we see that each set contains a sizable number of cloud and non-cloud samples. Since we would like to incorporate this image split method with the cross validation function, we will incorporate the validation data which is half of image 2 into the training data and use the validation data along with the training data from images 1 and 3 to train and validate on K folds of data before checking how the trained model will perform on the separate test data from image 2.

```{r val-test-maps}
#source("image_split.R")
split_ims <- image_split(data[,-4], data[,4], K=8)

val_map <- ggplot(data=split_ims$val_data, aes(x=X, y=Y,
                                               color=factor(split_ims$val_labels))) +
  geom_point() +
  coord_quickmap(expand = F, clip = "on") +
  labs(title = "Validation Set") +
  scale_color_manual(labels = c("Not Cloud", "Unlabeled", "Cloud"),
                     values = c("chocolate3", "darkseagreen", "deepskyblue"),
                     name = "Label") +
  theme(legend.position = "none")
test_map <- ggplot(data=split_ims$test_data, aes(x=X, y=Y,
                                            color=factor(split_ims$test_labels))) +
  geom_point() +
  coord_quickmap(expand = F, clip = "on") +
  labs(title = "Test Set") +
  scale_color_manual(labels = c("Not Cloud", "Unlabeled", "Cloud"),
                     values = c("chocolate3", "darkseagreen", "deepskyblue"),
                     name = "Label") +
  theme(legend.position = "none")
grid.arrange(test_map, val_map, leg, nrow=1)
```

## Baseline

Before splitting the data, we first remove all observations with 0 labels which correspond to the unlabeled points because we are primarily focused on detecting the presence of clouds or no clouds in the images. Therefore, a binary classification task between the 1 and -1 labels is a more appropriate way of approaching this problem. Even if we approached this problem as a multi-classification task between 3 different labels and were predicting all 3 labels accurately, predicting the unlabeled points accurately is somewhat meaningless in the context of detecting the presence of clouds or no clouds in the images.

We can now split the data using the methods outlined above and run a "classifier" which just sets all points to label $-1$, indicating cloudlessness. For the block splitting method, we split each image into 64 blocks, for a total of 192 blocks, of which 30 are for testing, 30 are for validation, and 132 are for training. This method gives a validation set misclassification rate of $63.0\%$ and a test set misclassification rate of $65.3\%$, which is about in line with the overall rate of $-1$ labels being $35-36\%$. This classifier would yield high accuracy if the image data was unbalanced to be mostly cloudless images, but in a scenario where the majority of points are not labeled $-1$, it is not a particularly accurate classifier.

```{r binary-data-all}
X_cv <- data %>%
  filter(Label != 0) %>%
  mutate(Label = ifelse(
    Label == -1,
    0, Label
  ))
y_cv <- X_cv %>% pull(Label)
y_cv <- as.factor(y_cv)

#binary_pct <- X_cv %>%
#              count(Label) %>%
#              summarise(pct = 100*n/nrow(X_cv))
#rownames(binary_pct) <- c("Not Cloud", "Cloud")
#colnames(binary_pct) <- c("% of pixels")
#binary_pct %>%
#  kable(digits = 3,
#        caption = "Cloud vs Not Cloud Binary Percentages") %>%
#  kable_styling(latex_options = "HOLD_position", full_width = F)
```

```{r block-split-baseline}
set.seed(521)

binary_data = data %>%
  filter(Label != 0)

tl = binary_data %>%
  pull(Label)
td = binary_data %>%
  dplyr::select(-Label)


td$block = rep(0, nrow(td))
labels_block = numeric()
ims = distinct(td %>% dplyr::select(image)) %>% pull(image)
snum = 0
  
for (im in ims){
  xvals = td %>%
    filter(image == im) %>%
    pull(X) %>%
    unique()
  yvals = td %>%
    filter(image == im) %>%
    pull(Y) %>%
    unique()
  
  xsplit = split(sort(xvals),
                 cut(seq_along(xvals),
                     8,
                     labels = FALSE))
  
  ysplit = split(sort(yvals),
                 cut(seq_along(yvals),
                     8,
                     labels = FALSE))

  for (xs in xsplit){
    for (ys in ysplit){
      snum = snum + 1
      td = td %>%
        mutate(block = ifelse(
          ((image == im) & (X %in% xs) & (Y %in% ys)), 
          snum, block))
      labels_block = c(labels_block, 
                            rep(snum, 
                                nrow(td %>%
                                       filter((image == im) & 
                                                (X %in% xs) & 
                                                (Y %in% ys)))))
    }
  }
}

#Splitting the blocks
bks = sample(seq.int(from = 1, to = snum, by = 1))

tnum = floor(length(bks)/6)
tmin = snum - tnum

block_train_labels = tl[labels_block %in% bks[1:(tmin-30)]]
block_valid_labels = tl[labels_block %in% bks[(tmin-29):tmin]]
block_test_labels = tl[labels_block %in% bks[(tmin+1):snum]]

block_train = td %>% filter(block %in% bks[1:(tmin-30)])
block_valid = td %>% filter(block %in% bks[(tmin-29):tmin])
block_test = td %>% filter(block %in% bks[(tmin+1):snum])

invisible(mean(block_valid_labels == 1))
invisible(mean(block_test_labels == 1))

```

Next, we will run this trivial classifier on our image splitting method where we split image 2 in half for the validation and test sets. This method gives a validation set misclassification rate of $40.6\%$ and a test set misclassification rate of $43.2\%$, which is about in line with the overall rate of $-1$ labels being $38.9\%$ in the binary dataset (with unclassified points removed). Once again, this trivial classifier will have a high average accuracy if the majority of the points are $-1$ (cloudless). Since the majority of the points are cloudless in the validation and test sets, this trivial classifier appears to be fairly accurate even though it is just predicting the majority class each time.

```{r baseline-image-split}
binary_image_split <- image_split(X_cv, y_cv, K = 8)

val_labels <- binary_image_split$val_labels
val_trivial <- rep(0, length(val_labels))
#print(sum(val_labels == 0)/length(val_labels))
#print(sum(val_trivial != val_labels)/length(val_trivial))

test_labels <- binary_image_split$test_labels
test_trivial <- rep(0, length(test_labels))
#print(sum(test_labels == 0)/length(test_labels))
#print(sum(test_trivial != test_labels)/length(test_trivial))
```

## First Order Importance

```{r corrplot-binary}
corrplot::corrplot(cor(X_cv[, 4:12]), type="upper", tl.col="black")
```

In general, the best set of features should include features that are strongly correlated with the response, in this case label, and features that are weakly correlated with each other. Looking at the correlation plot in the exploratory data analysis section, NDAI and CORR are fairly strongly correlated with label and are fairly weakly correlated with each other. Although SD is weakly correlated with CORR which would make it a good feature to include with CORR, it may not be preferable because it is strongly correlated with NDAI and weakly correlated with label. Looking at the scatter plots of the 3 combinations of these 3 variables below, we see that the -1 and 1 labels corresponding to the non-cloud and cloud labels are more easily separated with NDAI and CORR or with log(SD) and CORR. Therefore, we believe that NDAI and CORR are the best 2 features to select out of the 3 constructed physical features because they are both strongly correlated with the response label, are fairly weakly correlated with each other, and lead to more separable groups of labels.

```{r scatter-compare}
scatter1 <- ggplot() +
  geom_point(aes(x=X_cv$NDAI, y=log(X_cv$SD), color=y_cv),
             alpha=0.5) +
  labs(x="NDAI", y="log SD") +
  scale_color_manual(labels = c("Not Cloud", "Cloud"),
                     values = c("chocolate3", "deepskyblue")) +
  theme(legend.position = "none")
scatter2 <- ggplot() +
  geom_point(aes(x=X_cv$NDAI, y=X_cv$CORR, color=y_cv),
             alpha=0.5) +
  labs(x="NDAI", y="CORR") +
  scale_color_manual(labels = c("Not Cloud", "Cloud"),
                     values = c("chocolate3", "deepskyblue")) +
  theme(legend.position = "none")
scatter3 <- ggplot() +
  geom_point(aes(x=log(X_cv$SD), y=X_cv$CORR, color=y_cv),
             alpha=0.5) +
  labs(x="log SD", y="CORR",
       color="Label") +
  scale_color_manual(labels = c("Not Cloud", "Cloud"),
                     values = c("chocolate3", "deepskyblue"))

bin_leg = ggpubr::get_legend(scatter3)

scatter3 = scatter3 +
  theme(legend.position="none")

grid.arrange(scatter1, scatter2, scatter3, bin_leg,
             nrow=1, ncol=4)
```

Now looking at the radiance angles, the most promising features include BF, AF, and AN because they are all fairly highly correlated with label. Although the correlation of BF with label is slightly weaker, it has a weaker correlation with CORR which may make it a better feature to combine with NDAI and CORR.

```{r mutual-information, message=FALSE}
library(infotheo)
mutual_info <- function(feature) {
  mutinformation(y_cv, discretize(feature))
}

mi_table <- apply(X_cv[, 5:ncol(data)], MARGIN=2, FUN=mutual_info)
mi_table <- data.frame(t(mi_table))
rownames(mi_table) <- "Entropy"
mi_table %>%
  kable(digits=3,
        caption="Mutual Information with Label") %>%
  kable_styling(latex_options="HOLD_position", full_width=F)
```

Above is a table of the mutual information between each feature and the response label. The entropy of the empirical probability distributions are computed to quantify the amount of information obtained about label when observing one of the features. NDAI is certainly a good feature to select because it has the highest mutual information with label by a large margin. Although SD has a higher mutual information than CORR, since it is highly correlated with NDAI and CORR still has a high mutual information, we prefer to choose CORR alongside NDAI. As expected, the BF, AF, and AN radiance angles have a higher mutual information with label than the DF and CF radiance angles. Since the AF and AN radiance angles only have a slightly higher mutual information than the BF radiance angle, we may prefer to choose the BF radiance angle because it has a weaker correlation with CORR compared to the AF and AN radiance angles. The next table shows the table of the mutual information between the combination of NDAI and CORR with BF, AF, and AN respectively to see which combination of the 3 variables results in the greatest mutual information. The combination with BF results in the highest mutual information. This further justifies combining the BF radiance angle with NDAI and CORR because not only is it most weakly correlated with CORR, but it also gives the highest mutual information when in combination with NDAI and CORR.

Therefore, we have chosen the 3 best features to be NDAI, CORR, and the BF radiance angle because they are all fairly strongly correlated with the response label, they are only moderately correlated with each other, and they all have a relatively high mutual information with the response label.

```{r mutual-information-2}
mi_bf <- mutinformation(y_cv, discretize(X_cv[, c("NDAI", "CORR", "BF")]))
mi_af <- mutinformation(y_cv, discretize(X_cv[, c("NDAI", "CORR", "AF")]))
mi_an <- mutinformation(y_cv, discretize(X_cv[, c("NDAI", "CORR", "AN")]))

mi_table2 <- data.frame(c(mi_bf, mi_af, mi_an))
mi_table2 <- data.frame(t(mi_table2))
rownames(mi_table2) <- "Entropy"
colnames(mi_table2) <- c("NDAI, CORR, BF", "NDAI, CORR, AF", "NDAI, CORR, AN")
mi_table2 %>%
  kable(digits=3,
        caption="Mutual Information with Label") %>%
  kable_styling(latex_options="HOLD_position", full_width=F)
```

## Generic Cross-Validation

We then wrote a function that allows us to pass in the name of a trivial classifier (or a function call to such a classifier), and a number of folds, and outputs the $k$-fold Cross-validation misclassification rate on the training data provided using the classifier. The function uses the block splitting method described in Section 2(a), with $k^2$ blocks per image. It then partitions the blocks into $k$ groups, and, for each partition, treats that partition as a test set while training the classifier on the other $k-1$ partitions. Finally, the function returns the mean of the misclassification rates of the classifiers predicting on each of the $k$ test partitions. The function also allows for optional arguments, which are then passed to the generic classifier, so as to allow for classifiers to be modified from their generic versions.

The function also allows for the image splitting method described in Section 2(a). Taking images 1 and 3 from the training set and half of image 2 from the validation set, it splits these 2.5 images into K blocks which become the K folds used to train and validate the model in the function. The other half of image 2 is set aside as the test set so that after cross validation, the classifier is trained on the whole training set and the validation set before predicting and evaluating the performance on the test set.

# Modeling

```{r image-split-CVs, include=FALSE, warning=FALSE, message=FALSE}
source("CVmaster.R")
cv_lr <- CVmaster(X_cv[, -c(4, 6, 8, 9, 11, 12)], y_cv, "glm",
                  K = 8, split = "image", type = "response",
                  family="binomial")
cv_lda <- CVmaster(X_cv[, -c(4, 6, 8, 9, 11, 12)], y_cv, "lda",
                   K = 8, split = "image", type = "class")
cv_qda <- CVmaster(X_cv[, -c(4, 6, 8, 9, 11, 12)], y_cv, "qda",
                   K = 8, split = "image", type = "class")
cv_tree <- CVmaster(X_cv[, -c(4, 6, 8, 9, 11, 12)], y_cv, "tree",
                   K = 8, split = "image", type = "vector")
cv_rf <- CVmaster(X_cv[, -c(4, 6, 8, 9, 11, 12)], y_cv, "randomForest",
                  K = 8, split = "image", type = "prob",
                  ntree = 5)
cv_gbm <- CVmaster(X_cv[, -c(4, 6, 8, 9, 11, 12)], y_cv, "gbm",
                   K = 8, split = "image", type = "response",
                   distribution = "adaboost", bag.fraction = 1)
```

```{r image-split-baseline, include=FALSE}
im_block_labels <- binary_image_split$training_data %>%
  group_by(block, Label) %>%
  summarise(n = n())

image_split_baseline <- c()
for (b in unique(im_block_labels$block)) {
  freqs <- im_block_labels %>%
    filter(block == b) %>%
    summarise(freq = n/sum(n)) %>%
    pull(freq)
  image_split_baseline <- c(image_split_baseline, freqs[2])
}
image_split_baseline <- c(image_split_baseline,
                          mean(image_split_baseline))
image_split_baseline <- c(image_split_baseline,
                          sum(binary_image_split$test_labels != 0)/
                            length(binary_image_split$test_labels))
```

```{r image-split-comparisons, message=FALSE}
model_comp <- cbind(cv_lr$CV_loss, cv_lda$CV_loss, 
                    cv_qda$CV_loss, cv_tree$CV_loss,
                    cv_rf$CV_loss, cv_gbm$CV_loss,
                    image_split_baseline)
colnames(model_comp) <- c("LR", "LDA", "QDA", "Tree", "RF", "GBM",
                          "Baseline")
model_comp %>%
  kable(digits = 3,
        caption = "Image Split Model Comparison") %>%
  kable_styling(latex_options = "HOLD_position", full_width = F)
```

```{r image-ROC-cutoffs}
lr_index <- which.max(cv_lr$roc_obj$specificities + cv_lr$roc_obj$sensitivities)
lda_index <- which.max(cv_lda$roc_obj$specificities + cv_lda$roc_obj$sensitivities)
qda_index <- which.max(cv_qda$roc_obj$specificities + cv_qda$roc_obj$sensitivities)
tree_index <- which.max(cv_tree$roc_obj$specificities + cv_tree$roc_obj$sensitivities)
rf_index <- which.max(cv_rf$roc_obj$specificities + cv_rf$roc_obj$sensitivities)
gbm_index <- which.max(cv_gbm$roc_obj$specificities + cv_gbm$roc_obj$sensitivities)
```

```{r image-ROC-comparison}
ggplot() +
  # lr ROC
  geom_line(aes(x = 1 - cv_lr$roc_obj$specificities,
                y = cv_lr$roc_obj$sensitivities,
                color = "lr")) +
  # lr cutoff
  geom_point(aes(x = 1 - cv_lr$roc_obj$specificities[lr_index],
                 y = cv_lr$roc_obj$sensitivities[lr_index],
                 color = "cutoff"),
             shape = 8) +
  # lda ROC
  geom_line(aes(x = 1 - cv_lda$roc_obj$specificities,
                y = cv_lda$roc_obj$sensitivities,
                color = "lda")) +
  # lda cutoff
  geom_point(aes(x = 1 - cv_lda$roc_obj$specificities[lda_index],
                 y = cv_lda$roc_obj$sensitivities[lda_index],
                 color = "cutoff"),
             shape = 8) +
  # qda ROC
  geom_line(aes(x = 1 - cv_qda$roc_obj$specificities,
                y = cv_qda$roc_obj$sensitivities,
                color = "qda")) +
  # qda cutoff
  geom_point(aes(x = 1 - cv_qda$roc_obj$specificities[qda_index],
                 y = cv_qda$roc_obj$sensitivities[qda_index],
                 color = "cutoff"),
             shape = 8) +
  # tree ROC
  geom_line(aes(x = 1 - cv_tree$roc_obj$specificities,
                y = cv_tree$roc_obj$sensitivities,
                color = "tree")) +
  # tree cutoff
  geom_point(aes(x = 1 - cv_tree$roc_obj$specificities[tree_index],
                 y = cv_tree$roc_obj$sensitivities[tree_index],
                 color = "cutoff"),
             shape = 8) +
  # rf ROC
  geom_line(aes(x = 1 - cv_rf$roc_obj$specificities,
                y = cv_rf$roc_obj$sensitivities,
                color = "rf")) +
  # rf cutoff
  geom_point(aes(x = 1 - cv_rf$roc_obj$specificities[rf_index],
                 y = cv_rf$roc_obj$sensitivities[rf_index],
                 color = "cutoff"),
             shape = 8) +
  # gbm ROC
  geom_line(aes(x = 1 - cv_gbm$roc_obj$specificities,
                y = cv_gbm$roc_obj$sensitivities,
                color = "gbm")) +
  # gbm cutoff
  geom_point(aes(x = 1 - cv_gbm$roc_obj$specificities[gbm_index],
                 y = cv_gbm$roc_obj$sensitivities[gbm_index],
                 color = "cutoff"),
             shape = 8) +
  labs(title = "Image Split Test ROC",
       x = "1 - Specificity", y = "Sensitivity",
       color = "ROC curves") +
  scale_color_manual(labels = c("Cutoffs",
                                paste0("GBM AUC = ", round(cv_gbm$roc_obj$auc, 3)),
                                paste0("LDA AUC = ", round(cv_lda$roc_obj$auc, 3)),
                                paste0("LR AUC = ", round(cv_lr$roc_obj$auc, 3)),
                                paste0("QDA AUC = ", round(cv_qda$roc_obj$auc, 3)),
                                paste0("RF AUC = ", round(cv_rf$roc_obj$auc, 3)),
                                paste0("Tree AUC = ", round(cv_tree$roc_obj$auc, 3))),
                     values = c("black",
                                "red",
                                "orange",
                                "yellow",
                                "green",
                                "blue",
                                "purple"))
```

```{r block-split-baseline}
bsplit = block_split(X_cv, y_cv, K=8, seed=521)
btraining_data = bsplit$training_data
blabels = bsplit$labels
bsnum = bsplit$snum
btraining_labels = bsplit$training_data %>% pull(Label)

#Splitting the blocks
bbks = seq.int(from = 1, to = bsnum, by = 1)
bbks = sample(bbks)
btnum = floor(length(bbks)/6) #Set ~1/6 for testing
btest_data = btraining_data %>%
  filter(block %in% bbks[((length(bbks)-btnum)+1):length(bbks)])
btest_labels = btraining_labels[(blabels %in% 
                                   bbks[((length(bbks)-btnum)+1):length(bbks)])]
bbks = bbks[1:(length(bbks)-btnum)]
btraining_data = btraining_data %>%
  filter(block %in% bbks)
btraining_labels = btraining_labels[blabels %in% bbks]
blabels = blabels[blabels %in% bbks]
folds = createFolds(bbks, k = 8)

block_split_baseline <- c()
for (f in folds) {
  y = btraining_labels[blabels %in% f]
  freq <- mean(y != 0)
  block_split_baseline <- c(block_split_baseline, freq)
}
block_split_baseline <- c(block_split_baseline,
                          mean(block_split_baseline))
block_split_baseline <- c(block_split_baseline,
                          sum(btest_labels != 0)/
                            length(btest_labels))
```

```{r block-split-CVs, message=FALSE}
cv_lr_b <- CVmaster(X_cv[, -c(4, 6, 8, 9, 11, 12)], y_cv, "glm",
                  K = 8, split = "block", type = "response",
                  family="binomial")
cv_lda_b <- CVmaster(X_cv[, -c(4, 6, 8, 9, 11, 12)], y_cv, "lda",
                  K = 8, split = "block", type = "class")
cv_qda_b <- CVmaster(X_cv[, -c(4, 6, 8, 9, 11, 12)], y_cv, "qda",
                  K = 8, split = "block", type = "class")
cv_tree_b <- CVmaster(X_cv[, -c(4, 6, 8, 9, 11, 12)], y_cv, "tree",
                  K = 8, split = "block", type = "vector")
cv_rf_b <- CVmaster(X_cv[, -c(4, 6, 8, 9, 11, 12)], y_cv, "randomForest",
                  K = 8, split = "block", type = "prob",
                  ntree = 5)
cv_gbm_b <- CVmaster(X_cv[, -c(4, 6, 8, 9, 11, 12)], y_cv, "gbm",
                  K = 8, split = "block", type = "response",
                  distribution = "adaboost", bag.fraction = 1)

model_comp_b <- cbind(cv_lr_b$CV_loss, cv_lda_b$CV_loss, 
                      cv_qda_b$CV_loss, cv_tree_b$CV_loss,
                      cv_rf_b$CV_loss, cv_gbm_b$CV_loss,
                      block_split_baseline)
colnames(model_comp_b) <- c("LR", "LDA", "QDA", "Tree", 
                            "RF", "GBM", "Baseline")
model_comp_b %>%
  kable(digits = 3,
        caption = "Block Split Model Comparison") %>%
  kable_styling(latex_options = "HOLD_position", full_width = F)
```

```{r block-ROC-cutoffs}
lr_b_index <- which.max(cv_lr_b$roc_obj$specificities + cv_lr_b$roc_obj$sensitivities)
lda_b_index <- which.max(cv_lda_b$roc_obj$specificities + cv_lda_b$roc_obj$sensitivities)
qda_b_index <- which.max(cv_qda_b$roc_obj$specificities + cv_qda_b$roc_obj$sensitivities)
tree_b_index <- which.max(cv_tree_b$roc_obj$specificities + cv_tree_b$roc_obj$sensitivities)
rf_b_index <- which.max(cv_rf_b$roc_obj$specificities + cv_rf_b$roc_obj$sensitivities)
gbm_b_index <- which.max(cv_gbm_b$roc_obj$specificities + cv_gbm_b$roc_obj$sensitivities)
```

```{r block-ROC-comparison}
ggplot() +
  # lr ROC
  geom_line(aes(x = 1 - cv_lr_b$roc_obj$specificities,
                y = cv_lr_b$roc_obj$sensitivities,
                color = "lr")) +
  # lr cutoff
  geom_point(aes(x = 1 - cv_lr_b$roc_obj$specificities[lr_b_index],
                 y = cv_lr_b$roc_obj$sensitivities[lr_b_index],
                 color = "cutoff"),
             shape = 8) +
  # lda ROC
  geom_line(aes(x = 1 - cv_lda_b$roc_obj$specificities,
                y = cv_lda_b$roc_obj$sensitivities,
                color = "lda")) +
  # lda cutoff
  geom_point(aes(x = 1 - cv_lda_b$roc_obj$specificities[lda_b_index],
                 y = cv_lda_b$roc_obj$sensitivities[lda_b_index],
                 color = "cutoff"),
             shape = 8) +
  # qda ROC
  geom_line(aes(x = 1 - cv_qda_b$roc_obj$specificities,
                y = cv_qda_b$roc_obj$sensitivities,
                color = "qda")) +
  # qda cutoff
  geom_point(aes(x = 1 - cv_qda_b$roc_obj$specificities[qda_b_index],
                 y = cv_qda_b$roc_obj$sensitivities[qda_b_index],
                 color = "cutoff"),
             shape = 8) +
  # tree ROC
  geom_line(aes(x = 1 - cv_tree_b$roc_obj$specificities,
                y = cv_tree_b$roc_obj$sensitivities,
                color = "tree")) +
  # tree cutoff
  geom_point(aes(x = 1 - cv_tree_b$roc_obj$specificities[tree_b_index],
                 y = cv_tree_b$roc_obj$sensitivities[tree_b_index],
                 color = "cutoff"),
             shape = 8) +
  # rf ROC
  geom_line(aes(x = 1 - cv_rf_b$roc_obj$specificities,
                y = cv_rf_b$roc_obj$sensitivities,
                color = "rf")) +
  # rf cutoff
  geom_point(aes(x = 1 - cv_rf_b$roc_obj$specificities[rf_b_index],
                 y = cv_rf_b$roc_obj$sensitivities[rf_b_index],
                 color = "cutoff"),
             shape = 8) +
  # gbm ROC
  geom_line(aes(x = 1 - cv_gbm_b$roc_obj$specificities,
                y = cv_gbm_b$roc_obj$sensitivities,
                color = "gbm")) +
  # gbm cutoff
  geom_point(aes(x = 1 - cv_gbm_b$roc_obj$specificities[gbm_b_index],
                 y = cv_gbm_b$roc_obj$sensitivities[gbm_b_index],
                 color = "cutoff"),
             shape = 8) +
  labs(title = "Block Split Test ROC",
       x = "1 - Specificity", y = "Sensitivity",
       color = "ROC curves") +
  scale_color_manual(labels = c("Cutoffs",
                                paste0("GBM AUC = ", round(cv_gbm_b$roc_obj$auc, 3)),
                                paste0("LDA AUC = ", round(cv_lda_b$roc_obj$auc, 3)),
                                paste0("LR AUC = ", round(cv_lr_b$roc_obj$auc, 3)),
                                paste0("QDA AUC = ", round(cv_qda_b$roc_obj$auc, 3)),
                                paste0("RF AUC = ", round(cv_rf_b$roc_obj$auc, 3)),
                                paste0("Tree AUC = ", round(cv_tree_b$roc_obj$auc, 3))),
                     values = c("black",
                                "red",
                                "orange",
                                "yellow",
                                "green",
                                "blue",
                                "purple"))
```

Since specificity is the true negative rate and sensitivity is the true positive rate, we would like cutoff values on the ROC curves that maximize both of them. To obtain the cutoff value of each ROC curve, we sum the specificity and the sensitivity at each threshold and then pick the threshold that has the largest sum. By doing so, we pick the cutoff value for each model that maximizes both the true negative rate and the true positive rate.

# Diagnostics

## Diagnostics of AdaBoost

Of the different classifiers we considered, one of the best performing ones was Generalized Boosted Models, specifically using AdaBoost exponential loss on binary outcome, where 0 indicated cloudlessness (changed from -1 because some modeling techniques need 0-1 binary outcomes) and 1 indicated clouds. In boosting models, there are 2 main hyperparameters to we are focused on estimating: the number of trees in the boosting model, and a shrinkage parameter applied to each tree. To determine the optimal parameters, we can fit a cross-validated model at different values of the hyperparameters and record the average misclassification error, as well as calculating the error on a test set with a model trained on the entire training set.

```{r hyperest-ntree-lr, eval=FALSE, message=FALSE}
ntrees_orig = c(1, 5, 10, 20, 25, 40, 50, 60, 75, 80, 100, 
                 120, 125, 140, 150, 160, 175, 180, 200, 
                 225, 250, 275, 300, 350, 400)
lr_orig = c(0.005, 0.01, 0.025, 0.0375, 0.05, 0.0625, 0.075, 0.1, 0.25, 0.5)

outs = list()
outs_b = list()

for (t in ntrees_orig) {
  for (l in lr_orig) {
    mod = CVmaster(X_cv[, -c(4, 6, 8, 9, 11, 12)], y_cv, "gbm",
                    K = 8, split = "image", type = "response",
                    distribution = "adaboost", bag.fraction = 1, n.trees = t,
                    shrinkage = l)
    mod_b = CVmaster(X_cv[, -c(4, 6, 8, 9, 11, 12)], y_cv, "gbm",
                    K = 8, split = "block", type = "response",
                    distribution = "adaboost", bag.fraction = 1, n.trees = t,
                    shrinkage = l)
    outs = list.append(outs, mod$CV_loss)
    outs_b = list.append(outs_b, mod_b$CV_loss)
  }
}

outs = plyr::ldply(outs, data.frame)
outs_b = plyr::ldply(outs_b, data.frame)


folds = rep(c("F1", "F2", "F3", "F4", "F5", "F6", "F7", "F8", "A", "T"), 
            length(ntrees_orig)*length(lr_orig))
ntrees = rep(ntrees_orig, each = 10*length(lr_orig))
lr = rep(rep(lr_orig, each = 10), length(ntrees_orig))

n_df = cbind(ntrees, lr, folds, outs_b, outs)

colnames(n_df) = c("ntrees", "lr", "folds", "block", "image")

write_csv(n_df, "parameter_estimation.csv")
```

```{r param_estim_plot}
n_df = read_csv("parameter_estimation.csv")

ng1 = n_df %>%
  mutate(lr = as.factor(lr)) %>%
  filter(folds %in% c("A", "T")) %>%
  ggplot() +
  geom_line(data = . %>% filter (folds == "A"), aes(x = ntrees, y = block, 
                                                    group = lr,
                                                    color = lr),
            linewidth = 1) +
  labs(title = "Block Split",
       x = "Number of Trees Fit",
       y = "Misclassification Error",
       color = "Learning Rate") +
  theme(legend.position = "None")

ng2 = n_df %>%
  mutate(lr = as.factor(lr)) %>%
  filter(folds %in% c("A", "T")) %>%
  ggplot() +
  geom_line(data = . %>% filter (folds == "A"), aes(x = ntrees, y = image,
                                                    group = lr,
                                                    color = lr),
            linewidth = 1) +
  labs(title = "Image Split", 
       x = "Number of Trees Fit",
       y = element_blank(),
       color = "Learning Rate")

ng_leg = ggpubr::get_legend(ng2)

ng2 = ng2 +
  theme(legend.position = "none")

grid.arrange(ng1,ng2, ng_leg ,nrow=1, widths = c(1,1,.5))

n_df %>%
  filter(folds == "A") %>%
  filter(block == min(block))

n_df %>%
  filter(folds == "A") %>%
  filter(image == min(image))
```
In total, 250 pairings of the two hyperparameters were trained on both split methods, with 25 distinct values for the number of trees and 10 distinct learning rate choices. In the block split method, at higher numbers of trees, the best performing learning rate is 0.01 with around 275 trees. For image split, higher trees meant significantly better performance with the 0.01 learning rate, compared to the other learning rates. However, the best parameter pair, for both split methods, by average cross-validated loss was 50 trees with learning rate of 0.075.

## Comparing Misclassification Errors

## Finding a Better Classifier

## Comparing Split Methods

## Conclusion

# References
